Hadoop

Setup:
======
1) Got the access for hdfs cluster from course provider themselves. 
	Host IP - 54.85.143.224
	User Name â€“ hirwuser984
2) Needed to change the permission of the pem file received from the course provider with "chmod 600 <path to file>".
3) Use ssh command to connect to the cluster. Eg: ssh -i hirwuser984.pem hirwuser984@54.85.143.224

To copy files from cluster:

1) All course materials are found in hirw-workshop folder. To copy any file/folder from cluster, use the below command

scp -i hirwuser984.pem -r hirwuser984@54.85.143.224:/hirw-workshop .

Command explained

scp is the command to copy.
-r recursive copy
:/hirw-workshop (folder to be copied)
. current directory (all files will be copied to the current directory, if we want to give different location in local machine, we can replace . with the location)

What is BigData?
==================
BigData is not only about volume. But it is about 3 Vs.

3Vs: Volume Velocity Variety

There are 3 factors to be considered to confirm if the problem is related to BigData. 
	Volume: In Tera, Peta bytes
	Velocity: The rate at which the data is growing. If your getting 1 GB of data every day, then it is a velocity
	Variety: Not only text. Several forms of data. Videos, audios, images etc

Is BigDAta really exist?

	The problem space: 1) Science (NASA generates 1.7 GB every Hr) 2) Govt 3) Private (Amazon, Google tracks every clicks the users make to provide personalized experiences)

Big Data Challenges:

	Storage: Need to store them efficiently
	Computational Efficiency: Not only about storing but storing in a way that is good for computing
	Data Loss: Due to corruption, it may fail. We need proper recovery act in place
	Time: How much time it takes to provide a solution by analyzing BigData.
	Cost: Lot of storage space, lot of computational power

Why RDBMS not a solution?

	1) RDBMS is not horizontally scallable
	2) with variety of data, RDBMS will struggle
	3) Quite expensive for low volume of data

Why Grid Computing is not solution? Operates data on parallel

	1) It is good for compute intensive tasks with low volume of data. But not good for huge volume of data
	2) GC requires good experience with low level programming. Not for mainstream

Hadoop - The solution

	1) Stores huge data
	2) Storage efficiency
	3) Good data recovery solution
	4) Low cost
	5) Horizontally scale
	6) Easy to learn for programmers and non programmers


Understanding BigData Problem:
===============================

Suppose if we need to calculate closing price of each stock every day for stocks listed in NSE right from inception. How much time we would need?

Suppose, if the data is of 1 TB size. First we need to load the data and for laoding 1TB it will take 2.5 hours given we write at 122MB/sec. 
After loading data, we need to compute it with an efficient program. Lets say the program will take 60 mins. This would take no less than 3 hrs to compute the results. Suppose if the DB is outside the computing machine, then network bandwidth is also an overhead.

How do we solve this problem, we break this 1TB of data into 100 blocks and store these blocks in different nodes and have the computation in the same machine. This is called Distributed Computing. There are many complexities involved in both storage layer and computation layer. Thats what Hadoop offers.

Hadoop has two core components:
	HDFS:	Hadoop distributed file system: takes care of all storage related issues. Splitting the data into blocks and replicating each blocks in more than one nodes and keep track of which block lies where.
	Map Reduce: is a programming model and hadoop implements map reduce which takes care of all computational complexities.

	Hadoop f/w takes care of bringing all intermediates results from every single node to a consolidated result.

What is Hadoop?
	Hadoop is a f/w for distributed processing of large data sets across clusters of commodity computers.

Commodity computers: No need for specialized hardware. Commodity servers are not cheap computers, they are enterprise level servers but they are less expensive.


Why another File System - HDFS?
=================================

While reading and write data to Hard disc in computer, we talk through the FileSystem.

Each Operating system has their own file system. Eg: Windows has NTFS. Linux has ext4. Each of the file system can allow a file to be in the size of Exa Bytes, 1 EB is 1024 Tera Bytes. So why we need HDFS.

The need is the capability of HDFS. HDFS can allow us to break a file into blocks and also allows replication and keep track of which block lies where.

Consider a case of 10 node cluster with ext4 file system. Normal ext4 file system does not have the dsitributed capability. It can have only in the local view meaning a node does not know what data is present in another node in the cluster. For this reason, a normal file system is called local file system. 

Now assume we have a file system that spans over all the clusters which has a view of all nodes in the cluster. That is what is called as Hadoop Distributed File Systems. Though all file system has the concept of blocks, it is different from hdfs concept of blocks. In HDFS, when you upload a file, the file is broken into blocks with per block taking the size of 128 MB (older version 64 MB). Also, HDFS takes care of replication and keeps track of blocks that lies in each node.  By default HDFS replicates a block into 3 nodes. Suppose if we upload a file of 700 MB, we will have (5 * 128 + 1 * 60) 6 blocks. HDFS will decide which nodes have these blocks and also decides on the replication.


Question: When you have HDFS what happens to Local File System? HDFS is by no means an alternate for the local file system. Still for reading and writing data to hard disc, we have to go via the local file system. OS still rely on local file system and does not care whether HDFS exist or not. HDFS still goes to local file system for accessing the blocks.

BLOCKS
=======

Suppose there is a file called test.txt of size 4bytes. The following will be the case

size of the file: 4b
size on disk:	4kb

4KB could be the block size. Hence the the block chosen to store the file will be of size 4 KB and the space(4KB- 4b) can not be used. 

NTFS

		File Size			Unused Space
		2KB				2KB
		8KB				0 (the file would have taken 2 blocks of 4KB size)
		13KB				3KB (the file would have taken 3 blocks and 3 KB from last block will be unused)


HDFS

Consider the block size of HDFS is configured to be 256 MB. And the file size is 1 MB, will that waste 255 MB? No, it wont. Because, HDFS still rely on the underlying File system for file storage. Hence, there will be no wastage. Suppose if the file is of size 1025KB, then it would have taken 257 Blocks of size 4 KB leaving 3 KB of last block unused.

If the blocks are stored by underlying file system, why the block size of HDFS is 128 MB and why not 4 KB? The answer is if the block size is huge the file will be stored in contiguous blocks making the write/read faster. 

If storing big files are advantageous, why break it into blocks? 
Each file system has a limitation of file size, in case of NTFS, a file can not be of size more than 16XB. If your file is of 17XB, then we can not store that in NTFS. But HDFS provides means to break the files into blocks and store it across nodes.
Also, Breaking and replicating files across several nodes provide high availability. Consider a cluster of 3 nodes with each node having copies of the file. Also, consider a cluster of 15 nodes, with copies of blocks of files. The chances of 3 nodes failing is higher than the chances of all 15 nodes failing. 


Working With HDFS
=================

All hdfs commands begin with 				hadoop fs

Normal command: ls (this will list content of root directory)
HDFS command: hadoop fs -ls (will list a different content, because it shows the view of hdfs cluster) Irrespective of any node you try this content, the content will be same.

To list root directory with path name: hadoop fs -ls /user/hirwuser984
To list root directory: hadoop fs -ls (by default, the listing happens from root)
To create a directory: hadoop fs -mkdir sample-folder

To copy files from local FS: hadoop fs -copyFromLocal <file path in lfs> <destination location>

hadoop fs -copyFromLocal  /hirw-starterkit/hdfs/commands/dwp-payments-april10.csv diva-test

To copy file from one directory to another directory:  hadoop fs -cp diva-test/sample.txt diva-test2
To move file from one directory to another: hadoop fs -mv diva-test/dwp-payments-april10.csv div

Replication Factor: Be default any file is replicated 3 times in hdfs.
Eg: 
hirwuser984@ip-172-31-45-217:~$ hadoop fs -ls div/dwp-payments-april10.csv
-rw-r--r--   3 hirwuser984 hirwuser984    3326129 2018-08-27 12:31 div/dwp-payments-april10.csv    //In this line the number 3 represents the file dwp-payments-april10.csv is replicated thrice.

But we can override the replication factor in hdfs using the below command. The dfs.replication flag sets the replication factor for the file being copied.
hadoop fs -Ddfs.replication=2 -cp hadoop-test2/dwp-payments-april10.csv hadoop-test2/test_with_rep2.csv


File Permissions: The user who logged in drives the file permissions. Whatever files created by the current logged in user will be owned by that user.
fsck: file system check - This command is useful but needs sudo permissions. 

sudo -u hdfs hdfs fsck /user/ubuntu/input/yelp/yelp_academic_dataset_review.json -files -blocks -locations 

The above command list down ip address of nodes where the blocks are stored. It shows replication factor (repl), under replicated blocks, block name, status of the file. 

Where the blocks are stored in local file system? 

there is a config file called hdfs-site.xml has set of properties to configure hdfs. This file is defined up by hadoop administrator during cluster set up. In our cluster, this can be found in, /etc/hadoop/conf
dfs.datanode.data.dir is a place, where in we can mention the place where the blocks should be stored in the local file system. 


When a user asks for a file, hdfs should know how to construct the file back from the blocks, which means it should keep track of these blocks. This need some process or processes to keep hdfs functional. 


HDFS READ & WRITE
==================

copyFromLocal: Does a write to hdfs
copyToLocal: Does a read from hdfs

Two kinds of Nodes in HDFS: 1) NameNode (also called as master) 2) DataNode (also known as slave)

Namenode has meta information about the hdfs. List of files, list of blocks, who created the file etc. Does not store actual files. there will be one namenode per cluster.
Datanode: stores the actual files. There will be many datanodes.


Read Operation:

hadoop fs -cp from local node. When this command is executed, a java program run in the back ground.

The client program trying to read will contact the name node. If the repl.factor is 3, then for each block the namenode will return the list of all 3 datanodes. When namenode gives the list of datanodes, it sorts them in the ascending order based on the proximity to the client. The client can read the data from the closest one.

So which is a closest node? 
In a cluster, there will be nodes connected to one another and placed inside a rack. Again racks are connected to one another. A closest node is the client node itself. If the client does not have the block, the next closest is the node from the same rack, otherwise it is the node from other racks. Suppose if more than one nodes from same racks are present the one node is chosen at random.

Suppose during a read, if any node is not responding for a particular block, then the client wont go back to the same node during the current read operation. Now, the client will move to the next node from the list sent by the namenode.

Write Operation:
==================

When a client makes a request to write operation on hdfs, the namenode performs some checks like if the user permission and if the file is already present on the file system etc. If all checks are okay, it is going to proceed with block allocation. The namenode now has to pick the datanodes, which are not busy and has enough space to store blocks.

REplicaPlacement Strategy: To pick the datanodes, the namenode uses this replica replacement strategy. The following is the default replica replacement strategey.
	1) Same node as client
	2) Random node from another rack
	3) Random node from same rack.

Data pipeline: After the nodes are chosen, a data pipeline will be formed. The block will be written in the form of packets, the packet will be given to the first node in the chosen list of nodes and passed on to the next node in the sequence and so on. When all nodes have written the packet, an acknowledgement is sent back to the namenode. When all packets are written for a block, then the write operation will move on to the next block. 

Consider we write 3 blocks namely b1,b2,b3 and if the node2 goes down while writing a b2, first the packets for b1,b2,b3 are sent to node 3 so that it does not lose the blocks. Next the block name will be changed so that when the node2 comes back up it can not claim that it has the b1,b2,b3 (which could be incomplete). Say, blocks are renamed to b4,b5,b6 and when node2 comes back and say it has b1,b2,b3, the name node will find that those are non-existent blocks and ask node2 to remove them.

There is a property for minimum replication factor and that will be by default 1. In case of failure, as long as there is a minimum replication is achieved, the write will be considered successful. Later the namenode will coordinate with datanodes and arrange the missing nodes.